{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abef2a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch URL https://pypi.org/simple/hdfs/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/hdfs/ (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")) - skipping\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")) - skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/hdfs/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/hdfs/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/hdfs/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/hdfs/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/hdfs/\n",
      "ERROR: Could not find a version that satisfies the requirement hdfs (from versions: none)\n",
      "ERROR: No matching distribution found for hdfs\n",
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n"
     ]
    }
   ],
   "source": [
    "!pip install hdfs\n",
    "# ref : https://hdfscli.readthedocs.io/en/latest/quickstart.html#python-bindings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5058c012",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\CODE\\DE\\spark\\13_Extract.ipynb 셀 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhdfs\u001b[39;00m \u001b[39mimport\u001b[39;00m InsecureClient\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m client \u001b[39m=\u001b[39m InsecureClient(\u001b[39m'\u001b[39m\u001b[39mhttp://localhost:9870\u001b[39m\u001b[39m'\u001b[39m, user\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbig\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hdfs'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime as dt\n",
    "from hdfs import InsecureClient\n",
    "client = InsecureClient('http://localhost:9870', user='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95bf76",
   "metadata": {},
   "source": [
    "## hdfs로 부터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be17c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하명도 스파크 50\r\n",
      "홍길동 스파크 80\r\n",
      "임꺽정 스파크 60\r\n",
      "임요환 텐서플로우 100\r\n",
      "홍진호 텐서플로우 22\r\n",
      "홍진호 텐서플로우 22\r\n",
      "이윤열 텐서플로우 90\r\n",
      "최연성 장고 100최연성 장고 100\n"
     ]
    }
   ],
   "source": [
    "with client.read('/rdd/score.txt') as reader:\n",
    "  score = reader.read()\n",
    "score_str = bytes.decode(score)\n",
    "print(score_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f989c",
   "metadata": {},
   "source": [
    "## hdfs에 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3004a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/big/study/data/corona_data/sido_area.csv', encoding='CP949') as reader, client.write('/corona_data/loc/sido_area.csv') as writer:\n",
    "  for line in reader:\n",
    "        writer.write(line.encode('CP949'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a674f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "/corona_data/loc/sido_area.csv for client 127.0.0.1 already exists\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:389)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/corona_data/loc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/big/study/data/corona_data/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdfs/client.py:661\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     _logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to cleanup temporary folder.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    660\u001b[0m   \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdfs/client.py:650\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_threads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    649\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m path_tuple \u001b[38;5;129;01min\u001b[39;00m fpath_tuples:\n\u001b[0;32m--> 650\u001b[0m     \u001b[43m_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_tuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m   _map_async(n_threads, _upload, fpath_tuples)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdfs/client.py:581\u001b[0m, in \u001b[0;36mClient.upload.<locals>._upload\u001b[0;34m(_path_tuple)\u001b[0m\n\u001b[1;32m    578\u001b[0m     _progress(_local_path, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(_local_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m--> 581\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_temp_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdfs/client.py:527\u001b[0m, in \u001b[0;36mClient.write\u001b[0;34m(self, hdfs_path, data, overwrite, permission, blocksize, replication, buffersize, append, encoding)\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m AsyncWriter(consumer)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m   \u001b[43mconsumer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdfs/client.py:522\u001b[0m, in \u001b[0;36mClient.write.<locals>.consumer\u001b[0;34m(_data)\u001b[0m\n\u001b[1;32m    516\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    517\u001b[0m   method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m append \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    518\u001b[0m   url\u001b[38;5;241m=\u001b[39mloc,\n\u001b[1;32m    519\u001b[0m   data\u001b[38;5;241m=\u001b[39m(c\u001b[38;5;241m.\u001b[39mencode(encoding) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m _data) \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;28;01melse\u001b[39;00m _data,\n\u001b[1;32m    520\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[0;32m--> 522\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _to_error(res)\n",
      "\u001b[0;31mHdfsError\u001b[0m: /corona_data/loc/sido_area.csv for client 127.0.0.1 already exists\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:389)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n"
     ]
    }
   ],
   "source": [
    "client.upload('/corona_data/loc', '/home/big/study/data/corona_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d926b9f",
   "metadata": {},
   "source": [
    "## hdfs에 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7890db",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.write('/rdd/score.txt',  '최연성 장고 100'.encode('UTF-8'), append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27854b1",
   "metadata": {},
   "source": [
    "## hdfs 권한 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set_permission('/corona_data/loc', 777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13cfe0",
   "metadata": {},
   "source": [
    "## hdfs 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3e527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete('/corona_data/vaccine/corona_vaccine_2022-09-14.json')\n",
    "#client.delete('/corona_data/loc/sido_population.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5717cd0",
   "metadata": {},
   "source": [
    "## REST_API로 데이터를 호출해 HDFS에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa3264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "\n",
    "def execute_rest_api(method, url, headers, params):    \n",
    "    if method == 'get':\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "    elif method == 'post':\n",
    "        res = requests.post(url, params=params, headers=headers)\n",
    "    \n",
    "    if res == None or res.status_code != 200:\n",
    "        raise Exception('응답코드 : ' + str(res.status_code))\n",
    "\n",
    "    return res.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b503b",
   "metadata": {},
   "source": [
    "### 기준일자 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc50b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_std_day(befor_day):   \n",
    "    x = dt.datetime.now() - dt.timedelta(befor_day)\n",
    "    year = x.year\n",
    "    month = x.month if x.month >= 10 else '0'+ str(x.month)\n",
    "    day = x.day if x.day >= 10 else '0'+ str(x.day)  \n",
    "    return str(year)+ '-' +str(month)+ '-' +str(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50353ab8",
   "metadata": {},
   "source": [
    "### logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da676490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "co_logger = logging.getLogger('corona_api')\n",
    "handler = logging.FileHandler('./log/rest_api/'+cal_std_day(0)+'.log')\n",
    "co_logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_logger.error('테스트 에러 로그 입니다. 하하하')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00622bde",
   "metadata": {},
   "source": [
    "### api 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce140fc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'http://apis.data.go.kr/1352000/ODMS_COVID_04/callCovid04Api'\n",
    "service_key = '7E2bfvO9I4sNthgKkGQ317Fa599toGAa8AU0+V1vd3JuJW1k+Web3iaSbsp5PjKgNiPWdsfweZjLRVopDaQuLQ=='\n",
    "file_dir = '/corona_data/patient/'\n",
    "\n",
    "def create_param(befor_day):\n",
    "    return {\n",
    "        'serviceKey':service_key\n",
    "        ,'pageNo':'1'\n",
    "        ,'numOfRows':'500'\n",
    "        ,'apiType':'JSON'\n",
    "        ,'std_day':cal_std_day(befor_day)\n",
    "    }\n",
    "\n",
    "for i in range(1, 2):\n",
    "    params = create_param(i)\n",
    "    log_dict = {\n",
    "            \"is_success\":\"Fail\"\n",
    "        ,   \"type\":\"corona_patient\"\n",
    "        ,   \"std_day\":params['std_day']\n",
    "        ,   \"params\":params\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        res = execute_rest_api('get',url, {}, params)\n",
    "        file_name = 'corona_patient_' + params['std_day'] + '.json'\n",
    "        client.write(file_dir+file_name, res, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        log_dict['err_msg'] = e.__str__()\n",
    "        log_json = json.dumps(log_dict, ensure_ascii=False)\n",
    "        co_logger.error(log_json)\n",
    "        \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e95328",
   "metadata": {},
   "source": [
    "## 웹크롤링 hdfs 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889a53c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\CODE\\DE\\spark\\13_Extract.ipynb 셀 24\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     data\u001b[39m.\u001b[39mappend(tmp)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m res \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m:{\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#X32sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdesc\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m지역별 코로나 예방접종 인구 현황\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m    \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m:data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#X32sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/CODE/DE/spark/13_Extract.ipynb#X32sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m client\u001b[39m.\u001b[39mwrite(file_dir\u001b[39m+\u001b[39mfile_name, json\u001b[39m.\u001b[39mdumps(res, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_dir = '/corona_data/vaccine/'\n",
    "file_name = 'corona_vaccine_' + cal_std_day(1) + '.json'\n",
    "\n",
    "url = 'https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000'\n",
    "response_txt = execute_rest_api('get',url,{},{})\n",
    "soup = BeautifulSoup(response_txt, 'html.parser')\n",
    "trs = soup.select('#content > div.data_table.tbl_scrl_t > table > tbody > tr')\n",
    "\n",
    "cols = ['loc', 'v1', 'v2', 'v3', 'v4']\n",
    "data = []\n",
    "\n",
    "for idx, tr in enumerate(trs):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    th = tr.select('th')\n",
    "    tds = tr.select('td')\n",
    "    \n",
    "    rows = []\n",
    "    rows.append(th[0].text.replace(' ', '').replace('\\r\\n', ''))\n",
    "    \n",
    "    for idx, td in enumerate(tds):\n",
    "        if idx % 2 == 0:\n",
    "            continue            \n",
    "        rows.append(td.text.replace(' ', '').replace('\\r\\n', '').replace(',' , ''))\n",
    "        \n",
    "        tmp = dict(zip(cols, rows))\n",
    "    data.append(tmp)\n",
    "\n",
    "res = {\n",
    "    'meta':{\n",
    "        'desc':'지역별 코로나 예방접종 인구 현황',\n",
    "        'cols':{\n",
    "            'loc':'지역'\n",
    "            ,'v1':'1차접종자수'\n",
    "            ,'v2':'2차접종자수'\n",
    "            ,'v3':'3차접종자수'\n",
    "            ,'v4':'4차접종자수'\n",
    "        },\n",
    "        'std_day':cal_std_day(1)\n",
    "    },\n",
    "   'data':data\n",
    "}\n",
    "\n",
    "client.write(file_dir+file_name, json.dumps(res, ensure_ascii=False), encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da6d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
